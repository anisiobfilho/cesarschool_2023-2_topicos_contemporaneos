{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centro de Estudos e Sistemas Avançados do Recife\n",
    "\n",
    "Pós-graduação em Engenharia e Análise de Dados\n",
    "\n",
    "Disciplina de RNA e Deep Learning\n",
    "\n",
    "Professor: Silvan Ferreira da Silva Junior \n",
    " \n",
    "Grupo:\n",
    "* Anísio Pereira Batista Filho (apbf@cesar.school)\n",
    "* Carlos Cezar Lopes de Mendonça (cclm@cesar.school)\n",
    "* Rodolpho Victor França Valsconcelos (rvfv@cesar.school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando Modelos Pré-treinados com HuggingFace\n",
    "\n",
    "Hugging Face é uma plataforma que oferece uma ampla variedade de modelos pré-treinados para tarefas de processamento de linguagem natural (NLP). Usar esses modelos permite que você aplique técnicas avançadas de NLP sem precisar treinar um modelo do zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizadores\n",
    "\n",
    "Tokenização é o processo de converter texto em tokens, que são as unidades básicas que os modelos de NLP processam. A Hugging Face fornece tokenizadores para diferentes modelos, que geram tokens compatíveis com o modelo que será utilizado.\n",
    "\n",
    "Nesta seção, aprenderemos como carregar e utilizar tokenizadores com Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anisi\\anaconda3\\envs\\cesarschool310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\anisi\\anaconda3\\envs\\cesarschool310\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anisi\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\anisi\\anaconda3\\envs\\cesarschool310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\anisi\\anaconda3\\envs\\cesarschool310\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anisi\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, BertTokenizer\n",
    "\n",
    "# Carregando tokenizadores para GPT-2 e BERT\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens GPT-2: ['Art', 'ificial', 'Ġintelligence', 'Ġis', 'Ġthe', 'Ġfuture', '.']\n",
      "Tokens BERT: ['artificial', 'intelligence', 'is', 'the', 'future', '.']\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de tokenização\n",
    "input_text = \"Artificial intelligence is the future.\"\n",
    "gpt2_tokens = gpt2_tokenizer.tokenize(input_text)\n",
    "bert_tokens = bert_tokenizer.tokenize(input_text)\n",
    "\n",
    "print(f\"Tokens GPT-2: {gpt2_tokens}\")\n",
    "print(f\"Tokens BERT: {bert_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs GPT-2: [8001, 9542, 4430, 318, 262, 2003, 13]\n",
      "IDs BERT: [7976, 4454, 2003, 1996, 2925, 1012]\n"
     ]
    }
   ],
   "source": [
    "gpt2_ids = gpt2_tokenizer.convert_tokens_to_ids(gpt2_tokens)\n",
    "bert_ids = bert_tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "\n",
    "print(f\"IDs GPT-2: {gpt2_ids}\")\n",
    "print(f\"IDs BERT: {bert_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input GPT-2: {'input_ids': tensor([[8001, 9542, 4430,  318,  262, 2003,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "Input BERT: {'input_ids': tensor([[ 101, 7976, 4454, 2003, 1996, 2925, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "gpt2_input = gpt2_tokenizer(input_text, return_tensors=\"pt\")\n",
    "bert_input = bert_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Input GPT-2: {gpt2_input}\")\n",
    "print(f\"Input BERT: {bert_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos\n",
    "\n",
    "Modelos são as redes neurais que processam os tokens e geram saídas como texto ou embeddings. A Hugging Face disponibiliza uma variedade de modelos, como GPT-2 para geração de texto e BERT para tarefas como busca semântica.\n",
    "\n",
    "Aqui, aprenderemos como carregar e usar esses modelos para diferentes tarefas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando Modelos Pré-Treinados\n",
    "\n",
    "Para começar, vamos carregar modelos pré-treinados, como o GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Carregando o tokenizador GPT-2\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Carregando o modelo GPT-2\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=gpt2_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de Texto\n",
    "\n",
    "Geração de texto consiste em fornecer uma sequência inicial e permitir que o modelo continue gerando texto a partir dessa entrada. Vamos ver como fazer isso com o GPT-2.\n",
    "\n",
    "A geração de texto é uma das aplicações mais comuns de modelos como o GPT-2, que podem ser usados para completar frases, criar histórias ou mesmo gerar código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: {'input_ids': tensor([[ 818,  257,  995,  810, 9552]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Texto de entrada\n",
    "input_text = \"In a world where AI\"\n",
    "\n",
    "# Tokenizando a entrada\n",
    "input_ids = gpt2_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Input IDs: {input_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50])\n",
      "Output: tensor([[  818,   257,   995,   810,  9552,   318,   257,  1263,  1917,    11,\n",
      "           340,   338,  1593,   284,  1833,   703,   340,  2499,    13,   198,\n",
      "           198,   464,  1917,   318,   326,  9552,   318,   407,   257,  1917,\n",
      "           379,   477,    13,   632,   338,   257,  1917,   326,   460,   307,\n",
      "         16019,   416,   257,  1256,   286,  1180, 10581,    13,   198,   198]])\n"
     ]
    }
   ],
   "source": [
    "# Gerando o texto\n",
    "output = gpt2_model.generate(**input_ids, max_length=50)\n",
    "\n",
    "print(output.shape)\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world where AI is a big problem, it's important to understand how it works.\n",
      "\n",
      "The problem is that AI is not a problem at all. It's a problem that can be solved by a lot of different approaches.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decodificando o texto gerado\n",
    "generated_text = gpt2_tokenizer.decode(output[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para completar texto\n",
    "def complete_text(input_text, model=gpt2_model, tokenizer=gpt2_tokenizer, max_length=50):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    output = model.generate(**input_ids, max_length=max_length)\n",
    "    generated_text = tokenizer.decode(output[0])\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brazil is a country that has been a beacon of hope for the poor and the working class.\n",
      "\n",
      "The country's economic growth has been a boon for the country's poor, and the country's economy has been a boon for the country's middle\n"
     ]
    }
   ],
   "source": [
    "prediction = complete_text(\"Brazil is\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n"
     ]
    }
   ],
   "source": [
    "def generate_next_tokens(input_text, n_tokens=1, model=gpt2_model, tokenizer=gpt2_tokenizer):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    output = model.generate(input_ids, max_new_tokens=n_tokens)\n",
    "    generated_tokens = output[0][len(input_ids[0]):]  # Extrai apenas os novos tokens gerados\n",
    "    predicted_tokens = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return predicted_tokens.strip()\n",
    "\n",
    "input_text = \"Brazil is a country. Apple is a fruit. Python is a\"\n",
    "\n",
    "next_token = generate_next_tokens(input_text)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings de Texto\n",
    "\n",
    "Embeddings de texto são representações vetoriais de palavras ou frases que capturam o significado semântico. Esses embeddings podem ser utilizados em várias tarefas de NLP, como classificação de texto, busca semântica e agrupamento.\n",
    "\n",
    "Nesta seção, utilizaremos o BERT para gerar embeddings de texto e visualizar a similaridade entre diferentes frases. Também introduziremos conceitos como similaridade por cosseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Carregando o tokenizador BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Carregando o modelo BERT\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturando Embeddings\n",
    "\n",
    "Predição de embeddings consiste em fornecer um texto e utilizar um modelo encoder para prever a representação vetorial deste texto. Vamos ver como fazer isso com o BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Texto de entrada\n",
    "text = \"Artificial intelligence is the future.\"\n",
    "input_ids = bert_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Obtendo a representação do texto\n",
    "with torch.no_grad():\n",
    "    output = bert_model(input_ids)\n",
    "\n",
    "\n",
    "pooled_output = output.last_hidden_state.mean(dim=1)\n",
    "\n",
    "print(output.last_hidden_state.shape)\n",
    "print(pooled_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar embeddings usando BERT\n",
    "def get_embedding(text):\n",
    "    input_ids = bert_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids)\n",
    "    return outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medidas de Similaridade\n",
    "\n",
    "A similaridade por cosseno é uma métrica comum utilizada para medir a similaridade entre dois vetores no espaço de embeddings. Ela calcula o cosseno do ângulo entre dois vetores, onde um valor de 1 indica vetores idênticos e um valor de 0 indica vetores ortogonais (sem similaridade).\n",
    "\n",
    "A fórmula da similaridade por cosseno é:\n",
    "\n",
    "$\n",
    "\\text{similaridade}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "$\n",
    "\n",
    "Vamos calcular a similaridade entre diferentes frases usando embeddings gerados por BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de frases\n",
    "query = \"Artificial intelligence is transforming industries.\"\n",
    "doc1 = \"AI is changing the way we work.\"\n",
    "doc2 = \"Brazil is a country in South America.\"\n",
    "\n",
    "# Gerando embeddings\n",
    "query_embedding = get_embedding(query)\n",
    "doc1_embedding = get_embedding(doc1)\n",
    "doc2_embedding = get_embedding(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridade com doc1: 0.8325\n",
      "Similaridade com doc2: 0.5219\n"
     ]
    }
   ],
   "source": [
    "# Calculando similaridade por cosseno\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "similarity_doc1 = cos(query_embedding, doc1_embedding)\n",
    "similarity_doc2 = cos(query_embedding, doc2_embedding)\n",
    "\n",
    "print(f\"Similaridade com doc1: {similarity_doc1.item():.4f}\")\n",
    "print(f\"Similaridade com doc2: {similarity_doc2.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "Utilizando GPT-2, crie uma função que preveja o sujeito em uma frase. Por exemplo: Em \"John went to the store\", o sujeito é John."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_subject(text):\n",
    "  template=\"Alice goes to the gym: Alice. Bob was running: Bob. {text}:\"\n",
    "  input_text = template.format(text=text)\n",
    "  next_token = generate_next_tokens(input_text)\n",
    "  return next_token\n",
    "\n",
    "get_subject(\"John went to the store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2\n",
    "Em um sistema avançado de busca por livros, você deverá implementar uma função que faça uma busca semântica e retorne os 5 livros mais apropriados de acordo com a consulta do usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridade:  0.7111\n",
      "Similaridade:  A supernatural thriller where nightmares come to life.\n",
      "\n",
      "Similaridade:  0.6978\n",
      "Similaridade:  A darkly comic tale of life in the absurd.\n",
      "\n",
      "Similaridade:  0.6729\n",
      "Similaridade:  A suspenseful crime novel where the detective becomes the hunted.\n",
      "\n",
      "Similaridade:  0.6726\n",
      "Similaridade:  A psychological thriller that will mess with your mind.\n",
      "\n",
      "Similaridade:  0.6716\n",
      "Similaridade:  A detective story where the truth is stranger than fiction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "descriptions = [\n",
    "    \"A tale of love and loss set against the backdrop of war.\",\n",
    "    \"A gripping mystery where nothing is as it seems.\",\n",
    "    \"An epic fantasy adventure in a world of magic and dragons.\",\n",
    "    \"A heartwarming story of friendship and second chances.\",\n",
    "    \"A chilling thriller that will keep you on the edge of your seat.\",\n",
    "    \"A coming-of-age story about finding yourself and your place in the world.\",\n",
    "    \"A historical novel that brings the past to life with vivid detail.\",\n",
    "    \"A suspenseful crime novel where the detective becomes the hunted.\",\n",
    "    \"A dystopian future where one woman's rebellion could change everything.\",\n",
    "    \"A romantic comedy that will make you believe in love again.\",\n",
    "    \"A science fiction saga that explores the limits of human ingenuity.\",\n",
    "    \"A powerful drama about family, secrets, and redemption.\",\n",
    "    \"A journey through time to uncover hidden truths.\",\n",
    "    \"A modern fairy tale where dreams really do come true.\",\n",
    "    \"A dark fantasy filled with intrigue, betrayal, and forbidden magic.\",\n",
    "    \"A psychological thriller that will mess with your mind.\",\n",
    "    \"A poetic exploration of life, love, and everything in between.\",\n",
    "    \"A detective novel where every clue leads to more questions.\",\n",
    "    \"A story of survival and the strength of the human spirit.\",\n",
    "    \"A heart-pounding adventure in a world beyond our own.\",\n",
    "    \"A memoir of a life lived on the edge of society.\",\n",
    "    \"A romance that defies the boundaries of time and space.\",\n",
    "    \"A political thriller set in a world of corruption and power.\",\n",
    "    \"A fantasy epic that weaves together destiny and desire.\",\n",
    "    \"A mystery novel where the past refuses to stay buried.\",\n",
    "    \"A heartwrenching story of love, loss, and letting go.\",\n",
    "    \"A darkly comic tale of life in the absurd.\",\n",
    "    \"A science fiction adventure that questions what it means to be human.\",\n",
    "    \"A historical romance set in a time of revolution and change.\",\n",
    "    \"A supernatural thriller where nightmares come to life.\",\n",
    "    \"A journey of self-discovery in a world that demands conformity.\",\n",
    "    \"A story of forbidden love in a society bound by tradition.\",\n",
    "    \"A fast-paced action novel where every second counts.\",\n",
    "    \"A lyrical exploration of nature, solitude, and the passage of time.\",\n",
    "    \"A detective story where the truth is stranger than fiction.\",\n",
    "    \"A powerful saga of family, loyalty, and betrayal.\",\n",
    "    \"A fantastical journey through a land of myths and legends.\",\n",
    "    \"A tale of revenge, justice, and the price of power.\",\n",
    "    \"A story of hope in the face of overwhelming odds.\",\n",
    "    \"A quirky romance where opposites truly attract.\",\n",
    "    \"A sci-fi thriller that blurs the line between reality and illusion.\",\n",
    "    \"A historical epic that spans generations and continents.\",\n",
    "    \"A crime novel where the line between right and wrong is razor-thin.\",\n",
    "    \"A love story that unfolds in the most unexpected way.\",\n",
    "    \"A philosophical exploration of what it means to live a good life.\",\n",
    "    \"A gripping tale of survival in a post-apocalyptic world.\",\n",
    "    \"A romance that blossoms in the midst of chaos and war.\",\n",
    "    \"A detective novel that unravels the darkest secrets of the human soul.\",\n",
    "    \"A story of redemption and the power of forgiveness.\",\n",
    "    \"A fantasy adventure where a reluctant hero must save the world.\"\n",
    "]\n",
    "\n",
    "input_text = \"A horror novel\"\n",
    "\n",
    "# Gerando embeddings\n",
    "input_text_embedding = get_embedding(input_text)\n",
    "descriptions_embedding = [get_embedding(desc) for desc in descriptions]\n",
    "\n",
    "# Calculando similaridade por cosseno\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "similarity_input_text = [cos(input_text_embedding, desc_embedding) for desc_embedding in descriptions_embedding]\n",
    "\n",
    "top_similarities, top_indices = torch.topk(torch.tensor(similarity_input_text), 5)\n",
    "\n",
    "for sim, idx in zip(top_similarities, top_indices):\n",
    "  print(f\"Similaridade:  {sim.item():.4f}\")\n",
    "  print(f\"Similaridade:  {descriptions[idx]}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
